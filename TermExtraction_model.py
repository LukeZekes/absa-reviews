import os.path
import keras.models
from keras.layers import Dense, Dropout
from keras.losses import CategoricalCrossentropy
from keras.utils import to_categorical
import keras.regularizers
from gensim.models.doc2vec import Doc2Vec
import numpy as np
from extract_sentence_opinions import get_train_data, get_test_data, preprocess_text, generate_spans
from enums import TermClasses
from d2v import get_current_d2v_model

# Model takes an input of a vector generated by the d2v model, and outputs a class 0, 1, or 2 (defined by the TermClasses enum)
d2v = Doc2Vec.load(get_current_d2v_model())



def prep_data(data, max_num_invalid = 1000000000):
  num_invalid = 0
  max_span_len = 3
  prepped_data = []
  for entry in data:
    sentence = " ".join(preprocess_text(entry["sentence"]))
    aspect_tags = entry["aspect_terms"]
    opinion_tags = entry["opinion_terms"]
    sentence_spans = {
      'sentence': sentence,
      'spans': [],
      "opinion_pairs": entry["opinion_pairs"]
    }
    # Get all spans and their classes
    span_texts = generate_spans(sentence, max_span_len)
    spans = [[t, TermClasses["INVALID"].value] for t in span_texts]
    i = 0
    while i < len(spans):
      # Classify each span
      if spans[i][0] in aspect_tags: 
        spans[i][1] = TermClasses["ASPECT"].value
      elif spans[i][0] in opinion_tags:
        spans[i][1] = TermClasses["OPINION"].value
      else:
        if num_invalid > max_num_invalid:
          del spans[i]
          i -= 1
        else: num_invalid += 1
      i+=1
    sentence_spans["spans"] = spans
    prepped_data.append(sentence_spans)

  return prepped_data

def predict_type_from_span(span, model):
  w = len(span)
  vectorized_input = np.append(d2v.infer_vector(span.split()), w)
  # vectorized_input = vectorizer.infer_vector(span.split())
  reshaped_input = vectorized_input.reshape(1, -1)
  output = model.predict(reshaped_input, verbose = 0)
  return output

def load_TE_model(val = False):
  dirname = os.path.dirname(__file__)
  filename = os.path.join(dirname, r'models\\ATE\\good\\')
  model = keras.models.load_model(filename)
  if val:
    loaded_test_data = get_test_data()
    test_data = prep_data(loaded_test_data, max_num_invalid=500)
    vectorized_test_input = []
    test_input = []
    test_output = []
    for item in test_data:
      for span in item["spans"]:
        i = item["sentence"].lower().index(span[0])
        j = i + len(span)
        w = j - i + 1
        test_input.append(span[0])
        vectorized_test_input.append(np.append(d2v.infer_vector(span[0].split()), w))
        test_output.append(span[1])

    vectorized_test_input = np.array(vectorized_test_input)
    test_output = to_categorical(np.array(test_output), 3)
    model.evaluate(vectorized_test_input, test_output)
  return model

def create_model():
  # Create input and output sets from data
  d2v = Doc2Vec.load(get_current_d2v_model())
  loaded_train_data = get_train_data()
  loaded_test_data = get_test_data()
  train_data = prep_data(loaded_train_data, max_num_invalid=1000)
  vectorized_train_input = []
  train_input = []
  train_output = []
  for item in train_data:
    for span in item["spans"]:
      i = item["sentence"].lower().index(span[0])
      j = i + len(span)
      w = j - i + 1
      train_input.append(span[0])
      vectorized_train_input.append(np.append(d2v.infer_vector(span[0].split()), w))
      train_output.append(span[1])

  vectorized_train_input = np.array(vectorized_train_input)
  train_output = to_categorical(np.array(train_output), 3)

  test_data = prep_data(loaded_test_data, max_num_invalid=500)
  vectorized_test_input = []
  test_input = []
  test_output = []
  for item in test_data:
    for span in item["spans"]:
      i = item["sentence"].lower().index(span[0])
      j = i + len(span)
      w = j - i + 1
      test_input.append(span[0])
      vectorized_test_input.append(np.append(d2v.infer_vector(span[0].split()), w))
      test_output.append(span[1])

  vectorized_test_input = np.array(vectorized_test_input)
  test_output = to_categorical(np.array(test_output), 3)

  # Create model
  model = keras.models.Sequential()
  # model.add(Dropout(0.2))
  model.add(Dense(64, activation="sigmoid", input_shape=(101, )))
  # model.add(Dense(98, activation='sigmoid'))
  model.add(Dense(3, activation='softmax'))

  model.compile(optimizer='adam', loss=CategoricalCrossentropy(), metrics=['accuracy'])
  model.fit(x=vectorized_train_input, y=train_output, validation_data=(vectorized_test_input, test_output), epochs=100)
  # model.save("models/ATE/reindexed/")

  return train_data, train_input, train_output, test_data, test_input, test_output

# load_TE_model(val=True)